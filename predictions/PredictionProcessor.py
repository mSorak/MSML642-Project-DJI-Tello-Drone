import cv2
import torch
import numpy as np
from queue import Queue
from concurrent.futures import ThreadPoolExecutor


class PredictionProcessor:
    def __init__(self, yolo_model, midas_model, midas_transforms, target_labels, device, midas_device):
        """
        Initialize the PredictionProcessor.

        Args:
            yolo_model: Preloaded YOLO model for object detection.
            midas_model: Preloaded MiDaS model for depth estimation.
            midas_transforms: MiDaS preprocessing transforms.
            target_labels (list): List of target object labels for filtering.
            device: Device for YOLO (e.g., "cpu" or "cuda").
            midas_device: Device for MiDaS (e.g., "cpu" or "mps").
        """
        self.yolo_model = yolo_model.to(device)
        self.midas_model = midas_model.to(midas_device)
        self.midas_transforms = midas_transforms
        self.target_labels = target_labels
        self.device = device
        self.midas_device = midas_device
        self.prediction_results = {"detected_objects": [], "depth_map": None}

    def midas_depth_at_center(self, bounding_box, depth_map):
        """
        Get the depth value at the center of a bounding box.

        Args:
            bounding_box (tuple): The bounding box (x1, y1, x2, y2).
            depth_map (np.array): The depth map generated by MiDaS.

        Returns:
            float: The depth value at the center of the bounding box.
        """
        x1, y1, x2, y2 = bounding_box
        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2

        if 0 <= cy < depth_map.shape[0] and 0 <= cx < depth_map.shape[1]:
            return depth_map[cy, cx]
        else:
            return float('nan')  # Return NaN if the center is out of bounds

    def run_yolo(self, rgb_frame):
        """
        Run YOLO model on the input frame.

        Args:
            rgb_frame (numpy.ndarray): RGB frame.

        Returns:
            list: YOLO detections.
        """
        yolo_results = self.yolo_model(rgb_frame)
        return yolo_results.xyxy[0]  # YOLO detections

    def run_midas(self, rgb_frame):
        """
        Run MiDaS model for depth estimation on the input frame.

        Args:
            rgb_frame (numpy.ndarray): RGB frame.

        Returns:
            numpy.ndarray: Depth map.
        """
        input_batch = self.midas_transforms(rgb_frame).to(self.midas_device)
        with torch.no_grad():
            depth_map = self.midas_model(input_batch)
            depth_map = torch.nn.functional.interpolate(
                depth_map.unsqueeze(1),
                size=rgb_frame.shape[:2],
                mode="bilinear",
                align_corners=False,
            ).squeeze().cpu().numpy()
        return depth_map

    def process_frame(self, frame):
        """
        Process a single video frame to detect objects and estimate depth.

        Args:
            frame (numpy.ndarray): The input video frame.

        Returns:
            dict: Prediction results containing detected objects and depth map.
        """
        # Convert frame to RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Use ThreadPoolExecutor to run YOLO and MiDaS in parallel
        with ThreadPoolExecutor() as executor:
            yolo_future = executor.submit(self.run_yolo, rgb_frame)
            midas_future = executor.submit(self.run_midas, rgb_frame)

            # Wait for both tasks to complete
            detections = yolo_future.result()
            depth_map = midas_future.result()

        # Filter detections for target labels
        detected_objects = []
        for *box, confidence, class_id in detections:
            label = self.yolo_model.names[int(class_id)]
            if label in self.target_labels:
                x1, y1, x2, y2 = map(int, box)
                object_depth = self.midas_depth_at_center((x1, y1, x2, y2), depth_map)
                detected_objects.append({
                    "label": label,
                    "confidence": confidence.item(),
                    "depth": object_depth,
                    "bounding_box": (x1, y1, x2, y2)
                })

        # Update prediction results
        self.prediction_results = {"detected_objects": detected_objects, "depth_map": depth_map}
        return self.prediction_results

    def process_predictions(self, frame_queue):
        """
        Continuously processes frames from the frame queue for predictions.

        Args:
            frame_queue (queue.Queue): Queue holding video frames.
        """
        while True:
            if not frame_queue.empty():
                frame = frame_queue.get()
                if frame is not None:
                    self.process_frame(frame)

    def get_results(self):
        """
        Get the latest prediction results.

        Returns:
            dict: Latest prediction results.
        """
        return self.prediction_results
