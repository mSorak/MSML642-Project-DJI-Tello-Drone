{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to test YOLOv5 object detection and MiDaS depth estimation on a webcam feed\n",
    "# clone yolov5 git from https://github.com/ultralytics/yolov5.git (can also just directly download it)\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv5 model\n",
    "yolo_model = torch.hub.load('/Users/josephsketl/yolov5', 'yolov5s', source='local', pretrained=True)\n",
    "yolo_model.conf = 0.25  # Set confidence threshold\n",
    "\n",
    "# Load MiDaS model\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").small_transform\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "midas.to(device)\n",
    "yolo_model.to(device)\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video source.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert frame to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Run YOLO detection\n",
    "    yolo_results = yolo_model(rgb_frame)\n",
    "    detections = yolo_results.xyxy[0]\n",
    "\n",
    "    # Prepare frame for MiDaS\n",
    "    input_batch = midas_transforms(rgb_frame).to(device)\n",
    "\n",
    "    # Run MiDaS depth estimation\n",
    "    with torch.no_grad():\n",
    "        depth_map = midas(input_batch)\n",
    "        depth_map = torch.nn.functional.interpolate(\n",
    "            depth_map.unsqueeze(1),\n",
    "            size=rgb_frame.shape[:2],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        ).squeeze().cpu().numpy()\n",
    "\n",
    "    # Normalize depth map for visualization\n",
    "    depth_map_visual = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "    depth_map_color = cv2.applyColorMap(depth_map_visual, cv2.COLORMAP_MAGMA)\n",
    "\n",
    "    # Process detections\n",
    "    for *box, confidence, class_id in detections:\n",
    "        (x1, y1, x2, y2) = map(int, box)\n",
    "        label = yolo_model.names[int(class_id)]\n",
    "\n",
    "        # Get depth value at the center of the bounding box\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "        object_depth = depth_map[cy, cx]\n",
    "\n",
    "        # Display distance and bounding box\n",
    "        distance_text = f\"{label} ({confidence:.2f}): {object_depth:.2f} depth units\"\n",
    "        color = (0, 255, 0)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(frame, distance_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Display results\n",
    "    combined_frame = np.hstack((frame, depth_map_color))\n",
    "    cv2.imshow(\"YOLOv5 + MiDaS Object Detection and Depth Estimation\", combined_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
